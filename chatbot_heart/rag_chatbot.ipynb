{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d053ebf5",
   "metadata": {},
   "source": [
    "# Build a chatbot that gives advice on cardiovascular risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a348d",
   "metadata": {},
   "source": [
    "# Libraries and load secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85091ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# uv pip install -q llama-index-core\n",
    "# uv pip install -q llama-index-llms-groq\n",
    "# uv pip install -q llama-index-readers-file\n",
    "# uv pip install -q llama-index-embeddings-huggingface\n",
    "# uv pip install -q llama-index-embeddings-instructor\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6413d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keys\n",
    "import gradio as gr\n",
    "\n",
    "# Set the token as an environ variable\n",
    "os.environ[\"GROQ_API_KEY\"] = keys.GROQ_API_KEY#userdata.get(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = keys.HF_TOKEN #userdata.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a5d49",
   "metadata": {},
   "source": [
    "# Run only for setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c40fb",
   "metadata": {},
   "source": [
    "## Store relevant data\n",
    "Add all relevant data (.txt, .pdf etc) to contents/data manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d2490",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# # load in the documents\n",
    "# documents = SimpleDirectoryReader(\"./content/data\", required_exts=[\".pdf\"]).load_data(\n",
    "#     show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96f4ad",
   "metadata": {},
   "source": [
    "## Splitting the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ccd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# text_splitter = SentenceSplitter(chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "# docs = text_splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0433083",
   "metadata": {},
   "source": [
    "## Creating vectors with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f034bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# # embeddings\n",
    "# embedding_model = \"danielheinz/e5-base-sts-en-de\"#sentence-transformers/all-MiniLM-L6-v2\" # \n",
    "# # switched to work for german texts based on discussion https://discuss.huggingface.co/t/rag-embeddings-german-language/60840/4\n",
    "# embeddings_folder = \"./content/embedding_model/\" # if you're working locally instead of on colab\n",
    "\n",
    "# embeddings = HuggingFaceEmbedding(\n",
    "#     model_name=embedding_model, cache_folder=embeddings_folder\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c5764",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# vector_index = VectorStoreIndex.from_documents(\n",
    "#     documents, transformations=[text_splitter], embed_model=embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_index.storage_context.persist(persist_dir=\"./content/vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdb00d",
   "metadata": {},
   "source": [
    "# RAG - chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "model = \"llama-3.3-70b-versatile\"#\"allam-2-7b\"#\n",
    "\n",
    "llm = Groq(\n",
    "    model=model,\n",
    "    #max_tokens=100, #for allam as it is unable to be concise (as prompted)\n",
    "    token=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "embedding_model = \"danielheinz/e5-base-sts-en-de\"#sentence-transformers/all-MiniLM-L6-v2\" # \n",
    "# switched to work for german texts based on discussion https://discuss.huggingface.co/t/rag-embeddings-german-language/60840/4\n",
    "embeddings_folder = \"./content/embedding_model/\"\n",
    "\n",
    "embeddings = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model, cache_folder=embeddings_folder\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./content/vector_index\")\n",
    "vector_index = load_index_from_storage(storage_context, embed_model=embeddings)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=2) # answes get better with higher number, but also limit of tokens is more quickly reached\n",
    "\n",
    "prefix_messages = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content='''\n",
    "        Context: You are a friendly, supportive health assistant helping adults understand heart risk based on self-reported lifestyle factors (smoking, diet, exercise). You can reference clinical risk factors (blood pressure, cholesterol, family history) and lifestyle changes. Risk comes from an external ML model as “low,” “medium,” or “high.”\n",
    "        Objective: Explain lifestyle risk factors clearly and briefly, provide context about clinical factors, interpret risk categories, encourage achievable lifestyle changes, and advise seeing a doctor if risk is high or users ask clinical questions.\n",
    "        Behavior:\n",
    "        Keep answers concise (max 2 sentences).\n",
    "        Do not give medical diagnoses or instructions.\n",
    "        Highlight achievable lifestyle changes.\n",
    "        Include a short motivational nudge in every answer.\n",
    "        If a user asks about symptoms, medications, or is high risk, politely advise consulting a healthcare professional.\n",
    "        Remind users this tool is not a substitute for a doctor.\n",
    "        Tone:Friendly, encouraging, clear, concise, balanced between conversational and professional.'''\n",
    "     ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"Answer the question based only on the following context and previous conversation.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "rag_bot = ContextChatEngine(\n",
    "    llm=llm, retriever=retriever, memory=memory, prefix_messages=prefix_messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_2 = ContextChatEngine(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=ChatMemoryBuffer.from_defaults(),\n",
    "    prefix_messages=prefix_messages,\n",
    ")\n",
    "\n",
    "# Start the conversation loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Check for exit condition\n",
    "    if user_input.lower() == \"end\":\n",
    "        print(\"Ending the conversation. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Get the response from the conversation chain\n",
    "    response = bot_2.chat(user_input)\n",
    "    # Print the chatbot's response\n",
    "    print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe739f",
   "metadata": {},
   "source": [
    "# Create and Interface and make RAG chatbot available online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31882d85",
   "metadata": {},
   "source": [
    "### Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35951253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_bot_chat_interface(message, history):\n",
    "    # Call the chat engine with the new message\n",
    "    # The engine's internal memory handles the history\n",
    "    response = rag_bot.chat(message)\n",
    "\n",
    "    # Return the response text\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0f4dd",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theme = gr.themes.Soft(\n",
    "    primary_hue=\"rose\",\n",
    "    secondary_hue=\"rose\",\n",
    "    neutral_hue=\"rose\",\n",
    ")\n",
    "demo_chat_interface = gr.ChatInterface(\n",
    "    fn=rag_bot_chat_interface,\n",
    "    title=\"Love Your Heart\",\n",
    "    description=\"Ask me questions. I'll use my knowledge to respond.\",\n",
    "    theme=theme,#\"soft\",\n",
    "    # We can add example questions for the user to click on\n",
    "    examples=[\"What are risk factors for heart disease?\", \"How can I decrease my risk?\", \"Where can I find help?\"]\n",
    ")\n",
    "\n",
    "demo_chat_interface.launch() #share=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da49cb",
   "metadata": {},
   "source": [
    "# Using users self-reported data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b3df7",
   "metadata": {},
   "source": [
    "### Callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62256a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rag_bot_callback(message, chat_history, bmi_value):\n",
    "\n",
    "    # 1. Dynamically update the retriever with the new k value from the slider\n",
    "    # We cast to int() because the slider might pass a float\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "    # 2. Retrieve the context\n",
    "    nodes = retriever.retrieve(message)\n",
    "    context_str = \"\\n\\n\".join([n.get_content() for n in nodes])\n",
    "\n",
    "    # 3. Create a clean prompt including the context and the user's message\n",
    "    # We add our system prompts manually here\n",
    "    full_prompt = ('''\n",
    "        Context: You are a friendly, supportive health assistant helping adults understand heart risk based on self-reported lifestyle factors (smoking, diet, exercise). You can reference clinical risk factors (blood pressure, cholesterol, family history) and lifestyle changes. Risk comes from an external ML model as “low,” “medium,” or “high.”\n",
    "        Objective: Explain lifestyle risk factors clearly and briefly, provide context about clinical factors, interpret risk categories, encourage achievable lifestyle changes, and advise seeing a doctor if risk is high or users ask clinical questions.\n",
    "        Behavior:\n",
    "        Keep answers concise and succinct (1-2 sentences).\n",
    "        Do not give medical diagnoses or instructions.\n",
    "        Highlight achievable lifestyle changes.\n",
    "        Include a short motivational nudge in every answer.\n",
    "        If a user asks about symptoms, medications, or is high risk, politely advise consulting a healthcare professional.\n",
    "        Remind users this tool is not a substitute for a doctor.\n",
    "        Tone:Friendly, encouraging, clear, concise, balanced between conversational and professional.'''        \n",
    "        \"---------------------\\n\"\n",
    "        \"BMI: \" + str(bmi_value) +\"\\n\"\n",
    "        \"Context: \" + context_str + \"\\n\"\n",
    "        \"Question: \" + message\n",
    "    )\n",
    "\n",
    "    # 4. Generate the response (using the base LLM, not the chat engine)\n",
    "    response_obj = llm.complete(full_prompt)\n",
    "    bot_response = str(response_obj)\n",
    "\n",
    "    # 5. Update the chat history and return it to the chatbot component\n",
    "    # We must append the new [user_message, bot_response] pair\n",
    "    chat_history.append([message, bot_response])\n",
    "\n",
    "    # Gradio's gr.Chatbot component expects the *entire* updated history list to be returned.\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b314d74",
   "metadata": {},
   "source": [
    "### Building the UI with Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = gr.themes.Soft(\n",
    "    primary_hue=\"rose\",\n",
    "    secondary_hue=\"rose\",\n",
    "    neutral_hue=\"rose\",\n",
    ")\n",
    "with gr.Blocks(title=\"Custom RAG Bot UI\", theme=theme) as demo_custom:\n",
    "    gr.Markdown(\"<h1>Love your heart</h1>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # --- Sidebar/Parameter Column (scale=1 means it takes 1/5 of the width) ---\n",
    "        with gr.Column(scale=1, min_width=250):\n",
    "            gr.Markdown(\"## Your data\")\n",
    "            top_k_slider = gr.Slider(\n",
    "                minimum=15,\n",
    "                maximum=45,\n",
    "                step=1,\n",
    "                value=30,\n",
    "                label=\"BMI\",\n",
    "                #info=\"Controls how many relevant documents are retrieved.\"\n",
    "            )\n",
    "\n",
    "        # --- Main Chat Column (scale=4 means it takes 4/5 of the width) ---\n",
    "        with gr.Column(scale=4):\n",
    "            # The Chatbot component displays the history\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"AI Assistant\",\n",
    "                height=500,\n",
    "                # Provide an initial welcome message\n",
    "                value=[[None, \"Hello! I am your AI assistant. Ask me questions.\"]]\n",
    "            )\n",
    "\n",
    "            # The Textbox for user input\n",
    "            msg_input = gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Ask me something about heart health...\"\n",
    "            )\n",
    "\n",
    "            # The submission event\n",
    "            # When the user presses Enter in 'msg_input'...\n",
    "            msg_input.submit(\n",
    "                fn=custom_rag_bot_callback,  # ...call our function\n",
    "                # Pass in the current message, the chatbot history, and the slider value\n",
    "                inputs=[msg_input, chatbot, top_k_slider],\n",
    "                # The function's return value updates the chatbot component\n",
    "                outputs=[chatbot]\n",
    "            ).then(\n",
    "                # After submit, clear the textbox. We do this in a separate .then() step.\n",
    "                fn=lambda: None, # A simple function that returns None\n",
    "                inputs=None,\n",
    "                outputs=[msg_input], # The None value clears the textbox\n",
    "                queue=False # Run this step immediately\n",
    "            )\n",
    "\n",
    "# 3. Launch the Demo\n",
    "demo_custom.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3cdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
